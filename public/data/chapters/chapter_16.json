{
  "id": "chapter_16",
  "content": "\nChapter 16 \nManaging Security Operations \nTHE CISSP TOPICS COVERED IN THIS CHAPTER \nINCLUDE: \nDomain 2.0: Asset Security \n2.3 Provision information and assets securely \n2.3.1 Information and asset ownership \n2.3.2 Asset inventory (e.g., tangible, intangible) \n2.3.3 Asset management \nDomain 3: Security Architecture and Engineering \n3.1 Research, implement and manage engineering  processes using secure design principles \n3.1.2 Least privilege \n3.1.6 Segregation of Duties (SoD) \n3.5 Assess and mitigate the vulnerabilities of security  architectures, designs, and solution elements \n3.5.6 Cloud-based systems (e.g., Software as a Service \n(SaaS), Infrastructure as a Service (IaaS), Platform as  a Service (PaaS)) \n3.5.11 Serverless \nDomain 7: Security Operations \n7.3 Perform configuration management (CM) (e.g.,  provisioning, baselining, automation) \n7.4 Apply foundational security operations concepts \n7.4.1 Need-to-know/least privilege \n7.4.2 Segregation of Duties (SoD) and responsibilities \n7.4.3 Privileged account management \n7.4.4 Job rotation \n7.4.5 Service-level agreements (SLA) \n7.5 Apply resource protection \n7.5.1 Media management \n7.5.2 Media protection techniques \n7.5.3 Data at rest/data in transit \n7.8 Implement and support patch and vulnerability  management \n7.9 Understand and participate in change management  processes \n7.15 Address personnel safety and security concerns \n7.15.1 Travel \n7.15.2 Security training and awareness (e.g., insider  threat, social media impacts, two-factor  authentication (2FA) fatigue) \n7.15.3 Emergency management \n7.15.4 Duress \nDomain 8: Software Development Security \n8.4 Assess security impact of acquired software \n8.4.4 Managed services (e.g., enterprise applications) \n8.4.5 Cloud services (e.g., Software as a Service \n(SaaS), Infrastructure as a Service (IaaS), Platform as  a Service (PaaS)) \nSecurity operations includes a wide range of security foundational  concepts and best practices. These include several core concepts that  any organization needs to implement to provide basic security  protection. The first section of this chapter covers these concepts. \nResource protection ensures that information and assets are securely  provisioned when they're deployed and throughout their life cycle. \nConfiguration management ensures that systems are configured  correctly, and change management processes protect against outages  from unauthorized changes. Patch and vulnerability management  controls ensure that systems are up-to-date and protected against  known vulnerabilities. \nApply Foundational Security Operations \nConcepts \nThe primary purpose of IT security operations practices is to  safeguard assets such as information, systems, devices, facilities, and  applications. These practices help identify threats and vulnerabilities  and implement controls to reduce the risk to these assets. \nIn the context of IT security, due care and due diligence refer to  taking reasonable care to protect an organization's assets on an  ongoing basis. Senior management has a direct responsibility to  exercise due care and due diligence. Implementing the common  security operations concepts covered in the following sections, along  with performing periodic security audits and reviews, demonstrates a  level of due care and due diligence that will reduce senior  management's liability when a loss occurs. \nExam Tip \nRemember to keep the concepts of due care and due diligence top  of mind. Chapter 1, “Security Governance Through Principles and \nPolicies,” included a complete discussion of those topics. \nNeed-to-Know and Least Privilege \nNeed-to-know and the principle of least privilege are two standard  principles followed in any secure IT environment. They help protect  valuable assets by limiting access to these assets. Though they are  related and many people use the terms interchangeably, there is a  distinctive difference between the two. \nNeed-to-Know Access \nThe need-to-know principle imposes the requirement to grant users  access only to data or resources they need to perform assigned work  tasks. The primary purpose is to keep secret information secret. If  you want to keep a secret, the best way is to tell no one. If you're the  only person who knows it, you can ensure that it remains a secret. If  you tell a trusted friend, it might remain secret. However, your  trusted friend might tell someone else—such as another trusted  friend. The risk of the secret leaking out to others increases as more  and more people learn it. Limit the people who know the secret, and  you increase the chances of keeping it secret. \nNeed-to-know is commonly associated with security clearances, such  as a person having a Secret clearance. However, the clearance doesn't  automatically grant access to the data. As an example, imagine that \nSally has a Secret clearance. This indicates that she is cleared to  access Secret data. However, the clearance doesn't automatically  grant her access to all Secret data. Instead, administrators grant her  access to only the Secret data she has a need-to-know for her job. \nAlthough need-to-know is most often associated with military and  government agencies’ clearances, it can also apply in civilian  organizations. For example, database administrators may need  access to a database server to perform maintenance, but they don't  need access to all the data within the server's databases. Restricting  access on a need-to-know basis helps protect against unauthorized  access that could result in a loss of confidentiality. \nThe Principle of Least Privilege \nThe least privilege principle states that subjects are granted only the  privileges necessary to perform assigned work tasks and no more. \nKeep in mind that privilege in this context includes both permissions  to data and rights to perform systems tasks. For data, it includes  controlling the ability to read, write, create, alter, or delete data. \nLimiting and controlling privileges based on this concept protects  confidentiality and data integrity. If users can modify only those data  files that their work tasks require them to modify, it protects other  files’ integrity in the environment. \nThe least privilege principle relies on the  assumption that all users have a well-defined job description that  personnel understand. Without a specific job description, it is not  possible to know what privileges users need. \nThis principle extends beyond just accessing data, though—it also  applies to system access. For example, in many networks regular  users can log on to any computer in the network using a network  account. However, organizations commonly restrict this privilege by  preventing regular users from logging on to servers or restricting  users’ access to a single workstation. \nOrganizations sometimes violate this principle by adding all users to  the local Administrators group or granting root access to a computer. \nThis gives the users full control over the computer. However, regular  users rarely need this much access. When they have this much  access, they can accidentally (or intentionally) damage the system,  such as accessing or deleting valuable data. \nAdditionally, if a user logs on with full administrative privileges and  inadvertently installs malware, the malware can assume full  administrative privileges of the user's account. In contrast, if the user  logs on with a regular user account, malware can only assume the  regular account's limited privileges. Chapter 14, “Controlling and \nMonitoring Access,” discussed this in more depth within the context  of privilege escalation. \nLeast privilege is typically focused on ensuring that user privileges  are restricted, but it also applies to other subjects, such as  applications and system processes. For example, services and  applications often run under the context of an account specifically  created for the service or application. Historically, administrators  often gave these service accounts full administrative privileges  without considering the principle of least privilege. If attackers  compromise the application, they can potentially assume the service  account's privileges, granting the attacker full administrative  privileges. \nSegregation of Duties (SoD) and Responsibilities \nSegregation of duties (SoD) and responsibilities ensures that no  single person has total control over a critical function or system. This  is necessary to ensure that no single person can compromise the  system or its security. Instead, two or more people must conspire or  collude against the organization, which increases the risk for these  people. \nExam Tip \nSegregation of duties is also known as separation of duties. \nFortunately, both share the same SoD acronym. If you see either  phrase on the CISSP exam, know that they are synonymous. \nA segregation of duties policy creates a checks-and-balances system  where two or more users verify each other's actions and must work in  concert to accomplish necessary work tasks. This makes it more  difficult for individuals to engage in malicious, fraudulent, or  unauthorized activities and broadens the scope of detection and  reporting. In contrast, individuals may be more tempted to perform  unauthorized acts if they think they can get away with them. With  two or more people involved, the risk of detection increases and acts  as an effective deterrent. \nHere's a simple example. Movie theaters use segregation of duties to  prevent fraud. One person sells tickets. Another person collects the  tickets and doesn't allow entry to anyone who doesn't have a ticket. If  the same person collects the money and grants entry, this person can  allow people in without a ticket or pocket the collected money  without issuing a ticket. Of course, the ticket seller and the ticket  collector can get together and concoct a plan to steal from the movie  theater. This is collusion because it is an agreement between two or  more persons to perform some unauthorized activity. However,  collusion takes more effort and increases the risk to each of them. \nSegregation of duties policies help reduce fraud by requiring  collusion between two or more people to perform unauthorized  activity. \nSimilarly, organizations often break down processes into multiple  tasks or duties and assign these duties to different individuals to  prevent fraud. For example, one person approves payment for a valid  invoice, but someone else makes the payment. If one person  controlled the entire process of approval and payment, it would be  easy to approve bogus invoices and defraud the company. \nAnother way segregation of duties is enforced is by dividing the  security or administrative capabilities and functions among multiple  trusted individuals. When the organization divides administration  and security responsibilities among several users, no single person  has sufficient access to circumvent or disable security mechanisms. \nTwo-Person Control \nTwo-person control (sometimes called the two-man rule) requires  the approval of two individuals for critical tasks. For example, safe  deposit boxes in banks often require two keys. A bank employee  controls one key, and the customer holds the second key. Both keys  are required to open the box, and bank employees allow a customer  access to the box only after verifying the customer's identification. \nUsing two-person controls within an organization ensures peer  review and reduces the likelihood of collusion and fraud. For  example, an organization can require two individuals within the  company (such as the chief financial officer and the chief executive  officer) to approve key business decisions. \nAdditionally, some privileged activities can be configured so that  they require two administrators to work together to complete a task. \nAs an example, some privilege access management (PAM) solutions  create special administrative accounts for emergency use only. The  password is split in half so that two people need to enter the  password to log on. \nSplit knowledge combines the concepts of segregation of duties and  two-person control into a single solution. The basic idea is that the  information or privilege required to perform an operation is divided  among two or more users. This ensures that no single person has  sufficient privileges to compromise the security of the environment. \nJob Rotation \nJob rotation (sometimes called rotation of duties) means that  employees rotate through jobs or rotate job responsibilities with  other employees. Using job rotation as a security control provides  peer review, reduces fraud, and enables cross-training. Cross-  training helps make an environment less dependent on any single  individual. \nA job rotation policy can act as both a deterrent and a detection  mechanism. If employees know that someone else will be taking over  their job responsibilities in the future, they are less likely to take part  in fraudulent activities. If they choose to do so anyway, individuals  taking over the job responsibilities later are likely to discover the  fraud. \nMandatory Vacations \nMany organizations require employees to take mandatory vacations  in one-week or two-week increments. This provides a form of peer  review and helps detect fraud and collusion. This policy ensures that  another employee takes over an individual's job responsibilities for  at least a week. If an employee is involved in fraud, the person taking  over the responsibilities is likely to discover it. \nMandatory vacations can act as both a deterrent and a detection  mechanism, just as job rotation policies can. Even though someone  else will take over a person's responsibilities for just a week or two,  this is often enough to detect irregularities. \nFinancial organizations are at risk of significant  losses from fraud by employees. They often use job rotation,  segregation of duties and responsibilities, and mandatory  vacation policies to reduce these risks. Combined, these policies  help prevent incidents and help detect them when they occur. \nPrivileged Account Management \nPrivileged account management (PAM) solutions restrict access to  privileged accounts or detect when accounts use any elevated  privileges. In this context, privileged accounts are administrator  accounts or any accounts that have specific elevated privileges. This  can include help desk workers who have been granted limited  privileges to perform certain activities. \nIn Microsoft domains, this includes local administrator accounts \n(who have full control over a computer), users in the Domain \nAdmins group (who have full control of any computers in a domain),  and users in the Enterprise Admins group (who have full control over  all the domains in a forest). In Linux, this includes anyone using the  root account or granted root access via sudo. \nChapter 14 discussed some common Kerberos  attacks allowing attackers to take control of admin accounts. It  also discusses the sudo account. \nMicrosoft domains include a privileged access management (PAM)  solution that can restrict privileged access. It's based on a Just-in- \nTime administration principle. Users are placed in a privileged  group, but members of the group don't have elevated privileges. \nInstead, they request permission to use elevated privileges when they  need them. The PAM solution approves this request behind the  scenes and grants it within seconds by issuing a time-limited ticket. \nThe user only has elevated privileges for a specific time, such as 15  minutes. After the time is up, the ticket expires. This approach  thwarts common Kerberos attacks because the tickets quickly expire. \nEven if an attacker harvests one of these tickets, it is unusable. \nOn a more basic level, privileged account management monitors  actions taken by privileged accounts. This includes creating new user  accounts, adding new routes to a routing table, altering the  configuration of a firewall, and accessing system log and audit files. \nMonitoring ensures that users granted these privileges do not abuse  them. \nMonitoring special privileges is combined with other  basic principles, such as least privilege and segregation of duties  and responsibilities. Principles such as least privilege and  segregation of duties help prevent security policy violations, and  monitoring helps to deter and detect any violations that occur  despite the use of preventive controls. \nEmployees filling these privileged roles are usually trusted  employees. However, there are many reasons why an employee can  change from a trusted employee to a disgruntled employee or  malicious insider. Reasons that can change a trusted employee's  behavior can be as simple as a lower-than-expected bonus, a negative  performance review, or just a personal grudge against another  employee. However, by monitoring usage of special privileges, an  organization can deter an employee from misusing the privileges and  detect the action if a trusted employee does misuse them. \nMany automated tools are available that can monitor the usage of  special privileges. When an administrator or privileged operator  performs one of these activities, the tool can log the event and send  an alert. Additionally, access review audits detect misuse of these  privileges. \nFor example, many attackers use PowerShell scripts to escalate their  privileges. By configuring a security information and event  management (SIEM) system to detect and send alerts on certain  events, it's possible to detect the use of malicious PowerShell scripts. \nThere's more to this than just looking for specific Event IDs (such as \nEvent ID 4104). After modifying registry entries, the SIEM can also  record an entire PowerShell script and look for commands that  attackers commonly use. Chapter 17, “Preventing and Responding to \nIncidents,” covers SIEM systems in more depth. \nDetecting APTs \nMonitoring the use of elevated privileges can also detect  advanced persistent threat (APT) activities. For example, the U.S. \nDepartment of Homeland Security (DHS) and the Federal Bureau  of Investigation (FBI) released a technical alert (TA17-239A)  describing the activities of an APT targeting energy, nuclear,  water, aviation, and some critical manufacturing sectors, along  with some government entities. \nThe alert details how attackers infected a single system with a  malicious phishing email or by exploiting server vulnerabilities. \nOnce they exploited a single system, they escalated their  privileges and began performing many common privileged  operations, including the following: \nAccessing and deleting logs \nCreating and manipulating accounts (such as adding new  accounts to the Administrators group) \nControlling communication paths (such as opening port \n3389 to enable the Remote Desktop Protocol and/or  disabling the host firewall) \nRunning various scripts (including PowerShell, batch, and \nJavaScript files) \nCreating and scheduling tasks (such as one that logged their  accounts out after 8 hours to mimic the behavior of a regular  user) \nMonitoring common privileged operations can detect these  activities early in the attack. In contrast, if the actions go  undetected, the APT can remain embedded in the network for  years. \nService-Level Agreements (SLAs) \nA service-level agreement (SLA) is an agreement between an  organization and an outside entity, such as a vendor. The SLA  stipulates performance expectations and often includes penalties if  the vendor doesn't meet these expectations. \nAs an example, many organizations use cloud-based services to rent  servers. A vendor provides access to the servers and maintains them  to ensure that they are available. The organization can use an SLA to  specify availability, such as with uptimes and downtimes. With this  in mind, an organization should have a clear idea of their  requirements when working with third parties and ensure that the \nSLA includes these requirements. \nIn addition to an SLA, organizations sometimes use a memorandum  of understanding (MOU). MOUs document the intention of two  entities to work together toward a common goal. Although a MOU is  similar to an SLA, it is less formal and doesn't include any monetary  penalties if one of the parties doesn't meet its responsibilities. \nAddress Personnel Safety and Security \nPersonnel safety concerns are an essential element of security  operations. It's possible to replace things such as data, servers, and  even entire buildings. In contrast, it isn't possible to replace people. \nWith that in mind, organizations should implement security controls  that enhance personnel safety. \nAs an example, consider the exit door in a data center controlled by a  pushbutton electronic cipher lock. If a fire results in a power outage,  does the exit door automatically unlock or remain locked? An  organization that values assets in the server room more than  personnel safety might decide to ensure that the door remains locked  when power isn't available. Doing so protects the physical assets in  the data center, but it also risks the lives of personnel within the  room because they won't be able to easily exit the room. In contrast,  an organization that values personnel safety over the data center's  assets will ensure that the locks unlock the exit door when power is  lost. \nDuress \nDuress systems are useful when personnel are working alone. For  example, a single guard might be guarding a building after hours. If a  group of people break into the building, the guard probably can't  stop them on their own. However, a guard can raise an alarm with a  duress system. A simple duress system is just a button that sends a  distress call. A monitoring entity receives the distress call and  responds based on established procedures. The monitoring entity  could initiate a phone call or text message back to the person who  sent the distress call. In this example, the guard responds by  confirming the situation. \nSecurity systems often include code words or phrases that personnel  use to verify that everything truly is okay or verify that there is a  problem. For example, a code phrase indicating everything is okay  could be “Everything is awesome.” If a guard inadvertently activated  the duress system and the monitoring entity responded, the guard  says, “Everything is awesome” and then explains what happened. \nHowever, if criminals apprehended the guard, the guard could skip  the phrase and instead make up a story of how the duress system was  accidentally activated. The monitoring entity would recognize that  the guard skipped the code phrase and send help. \nSome electronic cipher locks support two or more codes, such as one  for regular use and one to raise an alarm. Normally, employees  would enter a code (such as 1 2 3 4) to open the door to a secure area. \nIn a duress situation, they could enter a different code (such as 5 6 7 \n8) that would open the door and set off a silent alarm. \nTravel \nAnother safety concern is when employees travel because criminals  might target an organization's employees while they are traveling. \nTraining personnel on safe practices while traveling can enhance  their safety and prevent security incidents. This includes simple  things such as verifying a person's identity before opening the hotel  door. If room service is delivering complimentary food, a call to the  front desk can verify if this is valid or part of a scam. \nEmployees should also be warned about the many risks associated  with electronic devices (such as smartphones, tablets, and laptops)  when traveling. These risks include the following: \nSensitive Data Ideally, the devices should not contain any  sensitive data. This prevents the loss of data if the devices are lost or  stolen. If an employee needs this data while traveling, it should be  protected with strong encryption. \nMalware and Monitoring Devices There have been many  reported cases of malware being installed on systems while  employees were visiting a foreign country. Similarly, we have heard  firsthand accounts of physical monitoring devices being installed  inside devices after a trip to a foreign country. People might think  their devices are safe in a hotel room as they go out to a local  restaurant. However, this is more than enough time for someone  who otherwise looks like hotel staff to enter your room, install  malware in the operating system, and install a physical listening  device inside the computer. Maintaining physical control of devices  at all times can prevent these attacks. Additionally, security experts  recommend that employees do not bring their personal devices but  instead bring temporary devices to be used during the trip. After the  trip, these can be wiped clean and reimaged. \nFree Wi-Fi Free Wi-Fi often sounds appealing while traveling. \nHowever, it can easily be a trap configured to capture all the user's  traffic. As an example, attackers can configure a Wi-Fi connection as  an on-path attack , forcing all traffic to go through the attacker's  system. The attacker can then capture all traffic. A sophisticated on-  path attack (sometimes called a man-in-the-middle attack) can  create an HTTPS connection between the client and the attacker's  system and create another HTTPS connection between the attacker's  system and an internet-based server. From the client's perspective, it  looks like it is a secure HTTPS connection between the client's  computer and the Internet-based server. However, all the data is  decrypted and easily viewable on the attacker's system. Instead,  users should have a method of creating their own internet  connection, such as through a smartphone or with a mobile wireless  hotspot device. \nVPNs Employers should have access to virtual private networks \n(VPNs) that they can use to create secure connections. These can be  used to access resources in the internal network, including their  work-related email. \nEmergency Management \nEmergency management plans and practices help an organization  address personnel safety and security after a disaster. Disasters can  be natural (such as hurricanes, tornadoes, or earthquakes) or the  result of people's actions (such as fires, terrorist attacks, or  cyberattacks causing massive power outages), as discussed in \nChapter 18, “Disaster Recovery Planning.” Organizations will have  different plans depending on the types of natural disasters they are  likely to experience. The safety of personnel should be a primary  consideration during any disaster. \nSecurity Training and Awareness \nChapter 2, “Personnel Security and Risk Management Concepts,”  covered security training and awareness programs in greater depth. \nIf an organization has a training and awareness program in place, it's  relatively easy to add personnel safety and security topics. These  programs help ensure that personnel are aware of duress systems,  travel best practices, emergency management plans, and general  safety and security best practices. \nWhen addressing personnel safety and security, training programs  should stress the importance of protecting people. Military warships  travel into war zones during times of conflict, putting personnel at  risk. However, they also do endless training to protect lives. \nOrganizations rarely face the same level of risk but should still  prioritize the value of human lives. \nOf course, as you learned in Chapter 2, security training and  awareness programs should include comprehensive coverage of  cybersecurity topics as well. Some important topics to include are: \nInsider threat . Educate employees on the risks associated with  unauthorized access or misuse of company data by employees,  contractors, or business partners. Highlight the signs of  potential insider threats and the protocols for reporting  suspicious behavior. \nSocial media impacts . Address the risks and vulnerabilities  associated with oversharing on social media platforms. Teach  employees about potential social engineering attacks that  leverage publicly available information and the importance of  setting strict privacy settings. \nTwo-factor authentication (2FA) fatigue . This segment can  address the common issue where users become complacent or  irritated with 2FA, often trying to bypass or minimize its use. \nTraining should emphasize the importance of 2FA in protecting  both personal and organizational data, ways to make 2FA more  user-friendly, and the potential consequences of neglecting this  security measure. \nProvision Information and Assets Securely \nAn important consideration when provisioning information and  assets securely is asset management. Chapter 13, “Managing Identity  and Authentication,” covered provisioning and deprovisioning for  accounts as part of the identity and access provisioning life cycle. \nThis section focuses on hardware, software, and information assets. \nInformation and Asset Ownership \nChapter 5, “Protecting Security of Assets,” discussed the importance  of identifying and classifying information and assets. It also  discussed various data roles. As a reminder, the data owner is the  person who has ultimate organizational responsibility for the data. \nThis is a senior manager, such as the chief executive officer (CEO),  president, or department head. Similarly, senior managers are  ultimately responsible for other assets, such as hardware assets. \nConsider an IT department that manages servers. The IT department  owns these servers, and the senior management in the IT  department is responsible for protecting them. \nThe key point is that by identifying the assets’ owners, an  organization also identifies the individuals responsible for protecting  those assets. Data owners typically delegate data protection tasks to  others in the organization. For example, employees in the data  custodian security role typically perform daily tasks such as  implementing access controls, performing backups, and managing  data storage. \nAsset Management \nAsset management refers to managing both tangible and intangible  assets. This typically starts with inventories of assets, tracking the  assets, and taking additional steps to protect them throughout their  lifetime. \nTangible assets include hardware and software assets owned by the  company. Intangible assets include patents, copyrights, a company's  reputation, and other assets representing potential revenue. By  managing assets successfully, an organization prevents losses. \nMany organizations use an automated configuration management  system (CMS) to help with hardware asset management. The  primary purpose of a CMS is configuration management, discussed  later in this chapter. The CMS needs to connect to hardware systems  when checking configuration settings. While doing so, it verifies that  the system is still on the network and turned on. \nHardware Asset Inventories \nHardware assets are IT resources such as computers, servers,  routers, switches, and peripherals. Many organizations use databases  and inventory applications to perform inventories and track  hardware assets through the entire equipment life cycle. For  example, bar-code systems are available that can print bar codes to  place on equipment. The bar-code database includes relevant details  on the hardware, such as the model, serial number, and location. \nWhen the hardware is purchased, it is bar-coded before it is  deployed. On a regular basis, personnel scan all of the bar codes with  a bar-code reader to verify that the organization still controls the  hardware. \nA similar method uses radio frequency identification (RFID) tags. \nThese tags transmit information to RFID readers. Personnel place  the RFID tags on the equipment and use the RFID readers to  inventory the equipment. RFID tags and readers are more expensive  than bar codes and bar-code readers. However, RFID methods  significantly reduce the time needed to perform an inventory. \nBefore disposing of equipment, personnel sanitize it. Sanitizing  equipment removes all data to ensure that unauthorized personnel  do not gain access to sensitive information. When equipment is at  the end of its lifetime, it's easy for individuals to lose sight of the data  that it contains, so using checklists to sanitize the system is often  valuable. Checklists can include steps to sanitize hard drives,  nonvolatile memory, and removable media such as CDs, DVDs, and \nUSB flash drives within the system. NIST 800-88r1 and Chapter 5  have more information on procedures to sanitize drives. \nPortable media, such as USB drives, holding sensitive data is also  managed as an asset. For example, an organization can label portable  media with bar codes and use a bar-code inventory system to  complete inventories on a regular basis. This approach allows them  to inventory the media holding sensitive data on a regular basis. \nSoftware Asset Inventories \nSoftware assets are operating systems and applications. \nOrganizations pay for software, and license keys are routinely used to  activate the software. The activation process often requires  contacting a licensing server over the Internet to prevent piracy. If  the license keys are leaked outside the organization, it can invalidate  the organization's use. It's also important to monitor license  compliance to avoid legal issues. \nFor example, an organization could purchase a license key for five  software product installations but only install and activate one  instance immediately. If the key is stolen and installed on four  systems outside the organization, those activations will succeed. \nWhen the organization tries to install the application on internal  systems, the activation will fail. Any type of license key is highly  valuable to an organization and should be protected. \nSoftware licensing also refers to ensuring that systems do not have  unauthorized software installed. Many tools are available that can  inspect systems remotely to detect the system's details. This allows  them to identify unauthorized software running on systems, and  helps an organization ensure that it complies with software licensing  rules. \nIntangible Inventories \nOrganizations don't inventory intangible resources in the same way  as tangible inventories. However, an organization needs to keep  track of intangible assets to protect them. Because these are  intellectual assets (such as intellectual property, patents, trademarks,  a company's reputation, and copyrights) instead of physical assets,  it's difficult to assign them a monetary value. \nThe senior management team is typically the owner of these assets. \nThey attempt to determine the value of intangible assets by  estimating the benefits the assets will bring to the organization. As  an example, imagine a company sells a product based on a patent. \nThe revenue from these sales can be used to assign a value to the  patent. Utility and plant patents in the United States are valid for 20  years and design patents for 15 years, so this time frame can also be  used when calculating the value. The United States requires payment  of maintenance fees periodically to maintain the patent. Failing to  pay these fees can result in a loss of the patent, stressing the  importance of tracking patents. \nLarge organizations report the value of intangible assets on their  balance sheets using generally accepted accounting principles \n(GAAP). This helps them review their intangible assets at least  annually. \nApply Resource Protection \nOrganizations apply various resource protection techniques to  ensure that resources are provisioned securely and managed  throughout their life cycle. As an example, desktop computers are  often deployed using imaging techniques to ensure that they start in  a known secure state. Change management and patch management  techniques ensure that the systems are kept up-to-date with required  changes. Imaging, change management, and patch management  topics are discussed later in this chapter. \nInformation is stored on media, so an essential part of resource  protection is protecting media. This includes when storing media  and when the media reaches the end of its life cycle. \nMedia Management \nMedia management refers to the steps taken to protect media and  data stored on media. In this context, media is anything that can  hold data. It includes tapes, optical media such as CDs and DVDs,  portable USB drives, internal hard drives, solid-state drives, and \nUSB flash drives. Many portable devices, such as smartphones, fall  into this category because they include memory cards that can hold  data. Backups are often contained on tapes, so media management  directly relates to tapes. However, media management extends  beyond just backup tapes to any type of media that can hold data. It  also includes any type of hard-copy data. \nMedia Protection Techniques \nWhen media includes sensitive information, it should be stored in a  secure location with strict access controls to prevent losses due to  unauthorized access. Additionally, any location used to store media  should have temperature and humidity controls to prevent losses due  to corruption. \nMedia management can also include technical controls to restrict  device access from computer systems. As an example, many  organizations use technical controls to block the use of USB drives  and/or detect and record when users attempt to use them. In some  situations, a written security policy prohibits the use of USB flash  drives, and automated detection methods detect and report any  violations. \nThe primary risks from USB flash drives are  malware infections and data theft. A system infected with a virus  can detect when a user inserts a USB drive and infect it. When the  user inserts this infected drive into another system, the malware  attempts to infect the second system. Additionally, malicious  users can easily copy and transfer large amounts of data and  conceal the drive in their pocket. \nProperly managing media directly addresses confidentiality,  integrity, and availability. When media is marked, handled, and  stored properly, it helps prevent unauthorized disclosure (loss of  confidentiality), unauthorized modification (loss of integrity), and  unauthorized destruction (loss of availability). \nControlling USB Flash Drives \nMany organizations restrict the use of USB flash drives to specific  brands purchased and provided by the organization. This strategy  allows the organization to protect data on the drives and ensure  that the drives are not being used to inadvertently transfer  malicious software (malware) between systems. Users still have  the benefit of USB flash drives, but this practice reduces risk for  the organization without hampering the user's ability to use USB  drives. \nFor example, some organizations sell IronKey flash drives that  include multiple levels of built-in protection. Several  authentication mechanisms are available to ensure that only  authorized users can access data on the drive. Such drives protect  data with built-in AES 256-bit hardware-based encryption. Active  antimalware software on the flash drive helps prevent malware  from infecting the drive. \nSome products include additional management solutions,  allowing administrators to manage the devices remotely. For  example, administrators can reset passwords, activate auditing,  and update the devices from a central location. \nTape Media \nOrganizations commonly store backups on tapes, and tapes are  highly susceptible to loss due to corruption. As a best practice,  organizations should keep at least two copies of backups. They  should maintain one copy on-site for immediate usage if necessary  and store the second copy at a secure location off-site. If a  catastrophic disaster such as a fire destroys the primary location, the  data is still available at the alternate location. \nThe cleanliness of the storage area will directly affect the life span  and usefulness of tape media. Additionally, magnetic fields can act as  a degausser and erase or corrupt data on the tape. With this in mind,  tapes should not be exposed to magnetic fields that can come from  sources such as elevator motors and some printers. Here are some  useful guidelines for managing tape media: \nKeep new media in its original sealed packaging until it's needed  to protect it from dust and dirt. \nWhen opening a media package, take extra caution not to  damage the media in any way. This includes avoiding sharp  objects and not twisting or flexing the media. \nAvoid exposing the media to temperature extremes; it shouldn't  be stored close to heaters, radiators, air conditioners, or other  sources of extreme temperatures. \nDo not use media that has been damaged, exposed to abnormal  levels of dust and dirt, or dropped. \nMedia should be transported from one site to another in a  temperature-controlled vehicle. \nMedia should be protected from exposure to the outside  environment; avoid sunlight, moisture, humidity, heat, and cold. \nIt should be acclimated for 24 hours before use. \nAppropriate security should be maintained over media from the  point of departure to the secured off-site storage facility. Media  is vulnerable to damage and theft at any point during  transportation. \nAppropriate security should be maintained over media  throughout the lifetime of the media based on the classification  level of data on the media. \nConsider encrypting backups to prevent unauthorized disclosure  of data if the backup tapes are lost or stolen. \nMobile Devices \nMobile devices include laptops, smartphones, tablets, and  smartwatches. These devices have internal memory or removable  memory cards that can hold a significant amount of data. Data can  include email with attachments, contacts, and scheduling  information. Additionally, many devices include applications that  allow users to read and manipulate different types of documents. \nChapter 9, “Security Vulnerabilities, Threats, and Countermeasures,”  covered mobile devices in much more depth. The key is to remember  that mobile devices include data storage abilities. If they are storing  sensitive data, it's important to take steps to protect that data. \nManaging Media Life Cycle \nAll media has a useful but finite life cycle. Reusable media is subject  to a mean time to failure (MTTF) that is sometimes represented in  the number of times it can be reused or the number of years you can  expect to keep it. For example, some tapes include specifications  saying they can be reused as many as 250 times or last up to 30 years  under ideal conditions. However, many variables affect the lifetime  of media and can reduce these estimates. It's important to monitor  backups for errors and use them as a guide to gauge the lifetime in  your environment. When a tape begins to generate errors,  technicians should rotate it out of use. \nChapter 10, “Physical Security Requirements,”  covered MTTF in more depth in the context of equipment failure. \nOnce backup media has reached its MTTF, it should be destroyed. \nThe classification of data held on the tape will dictate the method  used to destroy the media. Some organizations degauss highly  classified tapes when they've reached the end of their lifetime and  then store them until they can destroy the tapes. It's common to  destroy tapes in bulk shredders. \nChapter 5 discusses some of the security challenges with solid-state  drives (SSDs). Specifically, degaussing does not remove data from an \nSSD, and built-in erase commands often do not sanitize the entire  disk. Instead of attempting to remove data from SSDs, many  organizations destroy them. \nMTTF is different from mean time between failures \n(MTBF). MTTF is normally calculated for items that will not be  repaired when they fail, such as a tape. In contrast, MTBF refers  to the amount of time expected to elapse between failures of an  item that personnel will repair, such as a computer server. \nManaged Services in the Cloud \nCloud-based assets include any resources that an organization  accesses using cloud computing. You may see these referred to as  managed services . Cloud computing refers to on-demand access to  computing resources available from almost anywhere, and cloud  computing resources are highly available and easily scalable. \nOrganizations typically lease cloud-based resources from outside the  organization, but they can also host on-premises resources within  the organization. \nOne of the primary challenges with cloud-based resources hosted  outside the organization is that they are outside the organization's  direct control, making it more difficult to manage the risk. Although  the on-premises cloud provides the organization with much greater  control, hosting resources in the cloud offers convenience. \nSome cloud-based services only provide data storage and access. \nWhen storing data in the cloud, organizations must ensure that  security controls are in place to prevent unauthorized access to the  data. Additionally, organizations should formally define  requirements to store and process data stored in the cloud. For  example, the Department of Defense (DoD) Cloud Computing \nSecurity Requirements Guide (CC SRG) defines specific  requirements for U.S. government agencies to follow when  evaluating the use of cloud computing assets. This document  identifies computing requirements for assets labeled Secret and  below using six separate information impact levels. \nAll sensitive data should be encrypted. This includes data in transit  as it is sent to the cloud and data at rest while it's stored. The DoD \nCC SRG states that the customer should manage encryption,  including controlling all encryption keys. In other words, customers  should not use encryption controlled by the vendor. This eliminates  risks related to insider threats at the vendor and supports data  destruction using cryptographic erase methods. Cryptographic erase  methods permanently remove the cryptographic keys. If a strong  encryption method is used, cryptographic erase methods ensure that  data remains inaccessible. \nShared Responsibility with Cloud Service Models \nThere are varying levels of maintenance and security responsibilities  for assets, depending on the service model. This includes  maintaining the assets, ensuring that they remain functional, and  keeping the systems and applications up-to-date with current  patches. \nFigure 16.1 (derived from Figure 2 in the DoD CC SRG) shows how  vendors and customers share the maintenance and security  responsibilities for the three primary cloud service models. Refer to  it as you read through the following bullets. \nFIGURE 16.1 Cloud shared responsibility model \nSoftware as a Service (SaaS) Software as a service (SaaS)  models provide fully functional enterprise applications typically  accessible via a web browser. For example, Google's Gmail is a SaaS  application. The vendor (Google in this example) is responsible for  all maintenance of the SaaS services. SaaS comes with shared  responsibilities for data and applications. Customers may make  configuration changes to their Gmail accounts. Customers also share  responsibility for the data they keep and transmit via their Gmail  accounts. \nPlatform as a Service (PaaS) Platform as a service (PaaS)  models provide consumers with a computing platform, including  hardware, operating systems, and a runtime environment. The  runtime environment includes programming languages, libraries,  services, and other tools supported by the vendor. Customers deploy  applications that they've created or acquired, manage their  applications, and possibly modify some configuration settings on the  host. However, the vendor is responsible for maintenance of the host  and the underlying cloud infrastructure. \nInfrastructure as a Service (IaaS) Infrastructure as a service \n(IaaS) models provide basic computing resources to customers. This  includes servers, storage, and networking resources. Customers  install operating systems and applications and perform all required  maintenance on the operating systems and applications. The vendor  maintains the cloud-based infrastructure, ensuring that consumers  have access to leased systems. \nNIST SP 800-145—The NIST Definition of Cloud \nComputing , provides standard definitions for many cloud-based  services. This includes definitions for service models (SaaS, PaaS,  and IaaS), and definitions for deployment models (public,  private, community, and hybrid). NIST SP 800-144—Guidelines  on Security and Privacy in Public Cloud Computing , provides in-  depth details on security issues related to cloud computing. \nThe cloud deployment model also affects the breakdown of  responsibilities of the cloud-based assets. The four cloud deployment  models available are as follows: \nA public cloud model includes assets available for any  consumers to rent or lease and is hosted by an external CSP. \nService-level agreements can effectively ensure that the CSP  provides the cloud-based services at a level acceptable to the  organization. \nThe private cloud deployment model is used for cloud-based  assets for a single organization. Organizations can create and  host private clouds using their own on-premises resources. If so,  the organization is responsible for all maintenance. However, an  organization can also rent resources from a third party for  exclusive use of the organization. Maintenance requirements are  typically split based on the service model (SaaS, PaaS, or IaaS). \nA community cloud deployment model provides cloud-based  assets to two or more organizations that have a shared concern,  such as a similar mission, security requirements, policy, or  compliance considerations. Assets can be owned and managed  by one or more of the organizations. Maintenance  responsibilities are shared based on who is hosting the assets  and the service models. \nA hybrid cloud model includes a combination of two or more  clouds that are bound together by a technology that provides  data and application portability. Similar to a community cloud  model, maintenance responsibilities are shared based on who is  hosting the assets and the service models in use. \nAnything as a Service (XaaS) \nAnything as a service (XaaS) is the catchall term to refer to any  type of computing service or capability that can be provided to  customers through or over a cloud solution. Many service  providers that are rolling out new offerings to their clientele are  more often hosting the technology in a cloud solution rather than  on-premises equipment. This can enable rapid expansion,  scalability, high availability, and more when compared to the  previous means of deployment. \nOne area of growth in XaaS is security as a service (SECaaS),  where various forms of security services are being offered  through cloud solutions, including backup, authentication,  authorization, auditing/accounting, antimalware, storage, SIEM, \nIDS/IPS analysis, and monitoring as a service (MaaS). An \nSECaaS is also referred to as a managed service provider (MSP)  or a managed security service provider (MSSP). \nMSPs and MSSPs are third-party (often cloud-based) services  that provide remote oversight and management of on-premises \nIT or cloud IT. Some MSPs/MSSPs are general purpose, some  focus on specific IT areas (e.g., backup, security, storage,  firewall), and others are vertical management focused (e.g., legal,  medical, financial, government). \nScalability and Elasticity \nScalability refers to the ability of a system to handle additional  workloads by adding additional resources. As an example, imagine a  server has 16 GB of random access memory (RAM), but it can  support 64 GB of RAM. It's possible to shut down the server and add  additional RAM to scale it up. \nElasticity refers to a system's ability to add and remove resources  dynamically, based on increasing or decreasing load. As an example,  imagine an e-commerce server with 16 GB of RAM and a four-core  processor. Marketing launches an excellent advertising campaign  along with a sale. Suddenly, the server is overwhelmed with traffic. A  cloud provider that supports elasticity can dynamically add more \nRAM and processors to meet the increased workload. When the sale  ends and the workload decreases, the cloud provider can dynamically  remove the additional resources. \nChapter 9 covers virtualization concepts. Virtualization  technologies commonly support elasticity, too. \nA key point is that elasticity methods don't require shutting a system  down to add the resources. The resources are automatically added or  removed to match the demand. In contrast, scalability methods are  not typically automatic or dynamic, though they can be designed for  automatic scalability (horizontal and vertical scaling). They are  usually set up for manual scalability, which requires manual  intervention to add additional resources, such as an administrator  shutting down a system to add more RAM. \nAlthough the examples mention RAM and processor resources,  scalability and elasticity methods can extend a system's capability by  adding other resources. This includes adding more bandwidth, disk  space, or even more servers. \nServices Integration \nServices integration , cloud integration , systems integration , and  integration platform as a service (iPaaS) is the design and  architecture of an IT/IS solution that stitches together elements  from on-premises and cloud sources into a seamless productive  environment. The goals of services integration are to eliminate  data silos (a situation where data is contained in one area and  thus inaccessible to other applications or business units), expand  access, clarify processing visibility, and improve functional  connectivity of on-site and off-site resources. This can also be  viewed as an example of a software-defined data center (SDDC). \nServerless Architecture \nServerless architecture is a cloud computing concept where code is  managed by the customer and the platform (i.e., supporting  hardware and software) or server is managed by the cloud service  provider (CSP). There is always a physical server running the code,  but this execution model allows the software  designer/architect/programmer/developer to focus on the logic of  their code and not have to be concerned about the parameters or  limitations of a specific server. This is also known as function as a  service (FaaS) . \nApplications developed on serverless architecture are similar to  microservices, and each function is crafted to operate independently  and autonomously. This allows each function to be independently  scaled by the cloud service provider (CSP). This is distinct from PaaS,  where an entire execution environment or platform is spun up to  host an application, and it is always running, consuming resources  and racking up costs, even when it is not actively being used. With  serverless architecture or FaaS, the functions run only when called  and then terminate when their operations are completed, thus  minimizing costs. \nPerform Configuration Management (CM) \nConfiguration management (CM) helps ensure that systems are  deployed in a secure, consistent state and that they stay in a secure,  consistent state throughout their lifetime. Baselines and images are  commonly used to deploy systems. \nProvisioning \nProvisioning new systems refers to installing and configuring the  operating system and needed applications. Deploying operating  systems and applications using all of the defaults typically enables  many vulnerabilities. Instead, new systems should be configured to  reduce the vulnerabilities. \nA key consideration when provisioning a system is to harden it based  on its use. Hardening a system makes it more secure than the default  configuration and includes the following: \nDisable all unused services. As an example, a file server needs  services that allow users to access files, but file servers rarely use \nFTP. If the server is not using FTP, it should be disabled. \nClose all unused logical ports. These are often closed by  disabling unused services. \nRemove all unused applications. Some applications  automatically add additional applications. If these aren't used,  they should be removed. \nChange default passwords. Many applications have default  passwords for some accounts. Attackers know these, so the  passwords should be changed. \nBaselining \nA baseline is a starting point. In the context of configuration  management, it is the starting configuration for a system. An easy  way to think of a baseline is as a list of settings. An operating system  baseline identifies all the settings to harden specific systems. For  example, a baseline for a file server identifies the configuration  settings to harden the file server. Desktop computers would have a  different baseline. Although baselines provide a starting point,  administrators often modify them as needed for different systems  within their organization. \nUsing Images for Baselining \nMany organizations use images to deploy baselines. Figure 16.2  shows the process of creating and deploying baseline images in an  overall three-step process. Here are the steps: \nIn practice, more details are involved in this process,  depending on the tools used for imaging. For example, the steps  to capture and deploy images using one product are different  from the steps to capture and deploy images using another  product. \n1. An administrator starts by installing the operating system and  all desired applications on a computer (labeled as the baseline  system in the figure). The administrator then configures the  system with relevant security and other settings to meet the  organization's needs. Personnel then perform extensive testing  to ensure that the system operates as expected before  proceeding to the next step. \nFIGURE 16.2 Creating and deploying images \n2. Next, the administrator captures an image of the system using  imaging software and stores it on a server (labeled as an Image \nServer in Figure 16.2). It's also possible to store images on  external hard drives, USB drives, or DVDs. \n3. Personnel then deploy the image to systems as needed. These  systems often require additional configuration to finalize them,  such as giving them unique names. However, the overall  configuration of these systems is the same as the baseline  system. \nBaseline images improve the security of systems by ensuring that  desired security settings are always configured correctly. \nAdditionally, they reduce the amount of time required to deploy and  maintain systems, thus reducing the overall maintenance costs. \nDeployment of a prebuilt image can require only a few minutes of a  technician's time. If a user's system is corrupted, technicians can  redeploy an image in minutes, instead of taking hours to  troubleshoot the system or trying to rebuild it from scratch. \nOrganizations typically protect the baseline images to ensure that  they aren't modified. In a worst-case scenario, malware can be  injected into an image and then deployed to systems within the  network. \nAutomation \nIt's common to combine imaging with other automated methods for  baselines. In other words, administrators can create one image for all  desktop computers within an organization. They then use automated  methods to add additional applications, features, or settings for  specific groups of computers. For example, computers in one  department may have additional security settings or applications  applied through scripting or other automated tools. \nMicrosoft's operating systems include Group Policy. Administrators  can configure a Group Policy setting one time and automatically have  the setting apply to all the computers in the domain. Other Group \nPolicy settings can be configured to apply to all computers in a  group, such as all file servers or all the accounting department's  computers. \nIt's becoming common to make registry changes for some Windows  systems. As an example, attackers are using PowerShell in offensive  attacks quite often. Chapter 14 discusses PowerShell's use in  privilege escalation attacks. By modifying some registry settings,  administrators limit these attacks’ effectiveness and detect them  when they start. Some settings prevent an attacker from accessing \nPowerShell, and other settings enable additional logging so that  administrators can see what the attackers are doing with PowerShell. \nAdministrators can manipulate Group Policy settings to modify the  appropriate registry settings. \nManage Change \nDeploying systems in a secure state is a good start. However, it's also  essential to ensure that systems retain that same level of security. \nChange management helps reduce unanticipated outages caused by  unauthorized changes. \nThe primary goal of change management is to ensure that changes do  not cause outages. Change management processes ensure that  appropriate personnel review and approve changes before  implementation and ensure that personnel test and document the  changes. \nChanges often create unintended side effects that can cause outages. \nFor example, an administrator can change one system to resolve a  problem but unknowingly cause a problem in other systems. \nConsider Figure 16.3. The web server is accessible from the Internet  and accesses the database on the internal network. Administrators  have configured appropriate ports on Firewall 1 to allow internet  traffic to the web server and appropriate ports on Firewall 2 to allow  the web server to access the database server. \nFIGURE 16.3 Web server and database server \nA well-meaning firewall administrator may see an unrecognized  open port on Firewall 2 and decide to close it in the interest of  security. Unfortunately, the web server needs this port open to  communicate with the database server, so when the port is closed,  the web server will begin having problems. The help desk is soon  flooded with requests to fix the web server, and people begin  troubleshooting it. They ask the web server programmers for help,  and after some troubleshooting, the developers realize that the  database server isn't answering queries. They then call in the  database administrators to troubleshoot the database server. After a  bunch of hooting, hollering, blamestorming, and finger-pointing,  someone realizes that a needed port on Firewall 2 is closed. They  open the port and resolve the problem—at least until this well-  meaning firewall administrator closes it again or starts tinkering  with Firewall 1. \nOrganizations constantly seek the best balance between  security and usability. There are instances when an organization  makes conscious decisions to improve the performance or  usability of a system by weakening security. However, change  management helps ensure that an organization takes the time to  evaluate the risk of weakening security and compare it to the  benefits of increased usability. \nUnauthorized changes directly affect the A in the CIA Triad—  availability. However, change management processes allow various \nIT experts to review proposed changes for unintended side effects  before implementing the changes. These processes also give  administrators time to check their work in controlled environments  before implementing changes in production environments. \nAdditionally, some changes can weaken or reduce security. Imagine  an organization isn't using an effective access control model to grant  access to users. Administrators may not be able to keep up with the  requests for additional access. Frustrated administrators may decide  to add a group of users to an Administrators group within the  network. Users will now have all the access they need, improving  their ability to use the network, and they will no longer bother the  administrators with access requests. However, granting  administrator access in this way directly violates the least privilege  principle and significantly weakens security. \nMany of the configuration and change management  concepts in use today are derived from ITIL (formally an  acronym for Information Technology Infrastructure Library)  documents originally published by the United Kingdom. Even  though many of the concepts come from ITIL, organizations don't  need to adopt ITIL to implement change and configuration  management. \nChange Management \nA change management process ensures that personnel can perform a  security impact analysis. Experts evaluate changes to identify any  security impacts before personnel deploy the changes in a production  environment. \nChange management controls provide a process to control,  document, track, and audit all system changes. This includes changes  to any aspect of a system, including hardware and software  configuration. Organizations implement change management  processes through the life cycle of any system. \nCommon tasks within a change management process are as follows: \n1. Request the change . Once personnel identify desired changes,  they request the change. Some organizations use internal  websites, allowing personnel to submit change requests via a  web page. The website automatically logs the request in a  database, which allows personnel to track the changes. It also  allows anyone to see the status of a change request. \n2. Review the change . Experts within the organization review the  change. Personnel reviewing a change are typically from several  different areas within the organization. In some cases, they may  quickly complete the review and approve or reject the change. In  other cases, the change may require approval at a formal change  review board or change advisory board (CAB) after extensive  testing. Board members are the personnel who review the  change request. \n3. Approve/reject the change . Based on the review, these experts  then approve or reject the change. They also record the response  in the change management documentation. For example, if the  organization uses an internal website, someone will document  the results in the website's database. In some cases, the change  review board might require the creation of a rollback or backout  plan. This ensures that personnel can return the system to its  original condition if the change results in a failure. \n4. Test the change . Once the change is approved, it should be  tested, preferably on a nonproduction server. Testing helps  verify that the change doesn't cause an unanticipated problem. \n5. Schedule and implement the change . The change is scheduled so  that it can be implemented with the least impact on the system  and the system's customer(s). This may require scheduling the  change during off-duty or nonpeak hours. Testing should  discover any problems, but it's still possible that the change  causes unforeseen problems. Because of this, it's important to  have a rollback plan. This allows personnel to undo the change  and return the system to its previous state if necessary. \n6. Document the change . The last step is the documentation of the  change to ensure that all interested parties are aware of it. This  step often requires a change in the configuration management  documentation. If an unrelated disaster requires administrators  to rebuild the system, the change management documentation  provides them with information on the change. This ensures  that they can return the system to the state it was in before the  change. \nThere may be instances when an emergency change is required. For  example, if an attack or malware infection takes one or more systems  down, an administrator may need to make changes to a system or  network to contain the incident. In this situation, the administrator  still needs to document the changes. This ensures that the change  review board can review the change for potential problems. \nAdditionally, documenting the emergency change ensures that the  affected system(s) will include the new configuration if it needs to be  rebuilt. \nWhen the change management process is enforced, it creates  documentation for all changes to a system. This provides a trail of  information if personnel need to reverse the change. If personnel  need to implement the same change on other systems, the  documentation also provides a roadmap or procedure to follow. \nChange management control is a mandatory element for some  security assurance requirements (SARs) in the ISO Common \nCriteria. However, change management controls are implemented in  many organizations that don't require compliance with ISO Common \nCriteria. It improves the security of an environment by protecting  against unauthorized changes that result in unintentional losses. \nVersioning \nVersioning typically refers to version control used in software  configuration management. A labeling or numbering system  differentiates between different software sets and configurations  across multiple machines or at different points in time on a single  machine. For example, the first version of an application may be  labeled as 1.0. The first minor update would be labeled as 1.1, and the  first major update would be 2.0. This helps keep track of changes  over time to deployed software. \nAlthough most established software developers recognize the  importance of versioning and revision control with applications,  many new web developers don't recognize its importance. These web  developers have learned some excellent skills they use to create  awesome websites, but don't always recognize the importance of  underlying principles such as versioning control. If they don't control  changes through some type of versioning control system, they can  implement a change that effectively breaks the website. \nConfiguration Documentation \nConfiguration documentation identifies the current configuration of  systems. It identifies who is responsible for the system and its  purpose and lists all changes applied to the baseline. Years ago,  many organizations used simple paper notebooks to record this  information for servers, but it is much more common to store this  information in files or databases today. Of course, the challenge with  storing the documentation in a data file is that it can be inaccessible  during an outage. \nManage Patches and Reduce Vulnerabilities \nPatch and vulnerability management processes work together to help  protect an organization against emerging threats. Bugs and security  vulnerabilities are routinely discovered in operating systems and  applications. As they are discovered, vendors write and test patches  to remove the vulnerabilities. Patch management ensures that  appropriate patches are applied, and vulnerability management  helps verify that systems are not vulnerable to known threats. \nSystems to Manage \nIt's worth stressing that patch and vulnerability management doesn't  only apply to workstations and servers—it also applies to any  computing device with an operating system. Network infrastructure  systems such as routers, switches, firewalls, appliances (such as a  unified threat management appliance), and printers all include some  type of operating system. Some are Cisco-based, others are \nMicrosoft-based, and others are Linux-based. \nEmbedded systems are any devices that have a CPU, that run an  operating system, and that have one or more applications designed  to perform one or more functions. Examples include camera  systems, smart televisions, household appliances (such as burglar  alarm systems, wireless thermostats, and refrigerators), automobiles,  medical devices, and more. These devices are sometimes referred to  as the Internet of Things (IoT). \nThese devices may have vulnerabilities requiring patches. For  example, the massive distributed denial-of-service (DDoS) attack on \nDomain Name System (DNS) servers in late 2016 effectively took  down the Internet by preventing users from accessing dozens of  websites. Attackers reportedly used the Mirai malware to take  control of IoT devices (such as Internet Protocol [IP] cameras, baby  monitors, and printers) and join them to a botnet. Tens of millions of  devices sent DNS lookup requests to DNS servers, effectively  overloading them. Obviously, these devices should be patched to  prevent a repeat of this attack, but many manufacturers,  organizations, and owners don't patch IoT devices. Worse, many  vendors don't even release patches. \nFinally, if an organization allows employees to use mobile devices \n(such as smartphones and tablets) within the organizational  network, these mobile devices should be managed. MDM software  can deploy patches to mobile devices. \nPatch Management \nA patch is a blanket term for any type of code written to correct a bug  or vulnerability or to improve existing software performance. The  software can be either an operating system or an application. Patches  are sometimes referred to as updates, quick fixes, and hot fixes. In  the context of security, administrators are primarily concerned with  security patches, which are patches that affect a system's  vulnerability. \nEven though vendors regularly write and release patches, these  patches are useful only if they are applied. This may seem obvious,  but many security incidents occur simply because organizations  don't implement a patch management policy. As an example, one  attack in May 2017 exploited a vulnerability in an Apache Struts web  application that could have been patched in March 2017. \nAn effective patch management program ensures that systems are  kept up-to-date with current patches. These are the common steps  within an effective patch management program: \nEvaluate patches. When vendors announce or release patches,  administrators evaluate them to determine if they apply to their  systems. For example, a patch released to fix a vulnerability on a \nUnix system configured as a Domain Name System (DNS) server is  not relevant for a server running DNS on Windows. Similarly, a  patch released to fix a feature running on a Windows system is not  needed if the feature is not installed. \nTest patches. Whenever possible, administrators test patches on  an isolated nonproduction system to determine if the patch causes  any unwanted side effects. The worst-case scenario is that a system  will no longer start after applying a patch. For example, patches have  occasionally caused systems to begin an endless reboot cycle. They  boot into a stop error and keep trying to reboot to recover from the  error. If testing shows this on a single system, it affects only one  system. However, if an organization applies the patch to a thousand  computers before testing it, it could have catastrophic results. \nSmaller organizations often choose not to evaluate,  test, and approve patches but instead use an automatic method to  approve and deploy the patches. Windows systems include \nWindows Update, which makes this easy. However, larger  organizations usually take control of the process to prevent  potential outages from updates. \nApprove the patches. After administrators test the patches and  determine them to be safe, they approve the patches for deployment. \nIt's common to use a change management process (described earlier  in this chapter) as part of the approval process. \nDeploy the patches. After testing and approval, administrators  deploy the patches. Many organizations use automated methods to  deploy the patches. These can be third-party products or products  provided by the software vendor. \nVerify that patches are deployed. After deploying patches,  administrators regularly test and audit systems to ensure that they  remain patched. Many deployment tools include the ability to audit  systems. Additionally, many vulnerability assessment tools include  the ability to check systems to ensure that they have appropriate  patches. \nPatch Tuesday and Exploit Wednesday \nMicrosoft, Adobe, and Oracle regularly release patches on the  second Tuesday of every month, commonly called Patch Tuesday  or Update Tuesday. The regular schedule allows administrators  to plan for the release of patches so that they have adequate time  to test and deploy them. Many organizations that have support  contracts with Microsoft have advance notification of the patches  prior to Patch Tuesday. Some vulnerabilities are significant  enough that Microsoft releases them “out-of-band.” In other  words, instead of waiting for the next Patch Tuesday to release a  patch, Microsoft releases some patches earlier. \nAttackers realize that many organizations do not patch their  systems right away. Some malicious actors have reverse-  engineered patches to identify the underlying vulnerability and  then created methods to exploit the vulnerability. These attacks  often start within a day after Patch Tuesday, giving rise to the  term exploit Wednesday . \nHowever, many attacks occur on unpatched systems weeks,  months, and even years after vendors release the patches. In  other words, many systems remain unpatched, and attackers  exploit them much later than a day after the vendor released the  patch. \nVulnerability Management \nVulnerability management refers to regularly identifying  vulnerabilities, evaluating them, and taking steps to mitigate risks  associated with them. It isn't possible to eliminate risks. Similarly, it  isn't possible to eliminate all vulnerabilities. However, an effective  vulnerability management program helps an organization ensure  that it is regularly evaluating vulnerabilities and mitigating the  vulnerabilities that represent the greatest risks. Two common  elements of a vulnerability management program are routine  vulnerability scans and periodic vulnerability assessments. \nOne of the most common vulnerabilities within an  organization is an unpatched system, and so a vulnerability  management program will often work in conjunction with a patch  management program. In many cases, the duties of the two  programs are separated between different employees. One person  or group would be responsible for keeping systems patched, and  another person or group would be responsible for verifying that  the systems are patched. As with other segregation of duties  implementations, this approach provides checks and balances  within the organization. \nVulnerability Scans \nVulnerability scanners are software tools used to test systems and  networks for known security issues. A vulnerability scan enumerates \n(or lists) all the vulnerabilities in a system. Attackers use  vulnerability scanners to detect weaknesses in systems and networks,  such as missing patches or weak passwords. After they detect the  weaknesses, they launch attacks to exploit them. Administrators in  many organizations use the same types of vulnerability scanners to  detect vulnerabilities on their network. Their goal is to detect the  vulnerabilities and mitigate them before an attacker discovers them. \nScanners include the ability to generate reports identifying any  vulnerabilities they discover. The reports may recommend applying  patches or making specific configuration or security setting changes  to improve or impose security. These reports are passed on to  personnel performing patch management and managing system  settings. Simply recommending applying patches doesn't reduce the  vulnerabilities. Administrators need to take steps to apply the  patches. \nHowever, there may be situations where it isn't feasible or desirable  to do so. For example, if a patch fixing a minor security issue breaks  an application on a system, management may decide not to  implement the fix until developers create a workaround. The  vulnerability scanner will regularly report the vulnerability, even  though the organization has addressed the risk. \nManagement can choose to accept a risk rather than  mitigate it. Any risk that remains after applying a control is  residual risk. Any losses that occur from residual risk are the  responsibility of management. \nIn contrast, an organization that never performs vulnerability scans  will likely have many vulnerabilities. Additionally, these  vulnerabilities will remain unknown, and management will not have  the opportunity to decide which vulnerabilities to mitigate and which  ones to accept. \nCommon Vulnerabilities and Exposures \nVulnerabilities are commonly referred to using the Common \nVulnerabilities and Exposures (CVE) dictionary. The CVE dictionary  provides a standard convention used to identify and describe  vulnerabilities. MITRE maintains the CVE database, and you can  view it here: www.cve.org . \nMITRE looks like an acronym, but it isn't. The founders  do have a history as research engineers at the Massachusetts \nInstitute of Technology (MIT) and the name reminds people of  that history. However, MITRE is not a part of MIT. MITRE  receives funding from the U.S. Department of Homeland Security \n(DHS) and the Cybersecurity and Infrastructure Security Agency \n(CISA) to maintain the CVE database. \nPatch management and vulnerability management tools commonly  use the CVE dictionary as a standard when scanning for specific  vulnerabilities. As an example, CVE-2020-0601 identifies a  vulnerability in the Windows CryptoAPI ( Crypt32.dll ). Microsoft  patched this vulnerability in the January 2020 security update. \nThe CVE database makes it easier for companies that create patch  management and vulnerability management tools. They don't have  to expend any resources to manage the naming and definition of  vulnerabilities, but instead focus on methods used to check systems  for the vulnerabilities. \nSummary \nSeveral basic security principles are at the core of security operations  in any environment. These include need-to-know, least privilege,  segregation of duties (SoD) and responsibilities, job rotation and  mandatory vacations, privileged account management, and service-  level agreements (SLAs). Combined, these practices help prevent  security incidents from occurring and limit the scope of incidents  that do occur. \nWhen addressing personnel safety and security, safety of personnel  should always be a high priority. Duress systems allow guards to  raise silent alarms in response to emergencies, and emergency  management plans help the organization respond to disasters. \nTraveling presents unique risks to employees, such as the loss of  data, malware installed on unattended systems, and intercepted data  when using free Wi-Fi networks. Safety training and awareness  programs ensure that personnel know the various risks and ways to  mitigate them. Training and awareness programs should also  address other critical issues, including the insider threat, social  media use, and multifactor authentication fatigue. \nAsset management extends beyond media to any asset considered  valuable to an organization. This includes both tangible and  intangible assets. Tangible assets include hardware and software,  and organizations commonly inventory these assets to track them. \nIntangible assets include patents, trademarks, and copyrights, and  organizations track these assets as well. \nWith resource protection, media and other assets that contain data  are protected throughout their life cycle. Media includes anything  that can hold data, such as tapes, internal drives, portable drives, \nCDs and DVDs, mobile devices, memory cards, and printouts. Media  holding sensitive information should be marked, handled, stored,  and destroyed using methods that are acceptable within the  organization. \nManaged services in the cloud include any resources stored in or  accessed via the cloud. When negotiating with cloud service  providers, you must understand who is responsible for maintenance  and security. In general, the cloud service provider has the most  responsibility with software as a service (SaaS) resources, less  responsibility with platform as a service (PaaS) offerings, and the  least responsibility with infrastructure as a service (IaaS) offerings. \nCloud services commonly provide elasticity, which is the ability of  services to dynamically respond to changing workload requirements. \nChange and configuration management are two additional controls  that help reduce outages. Configuration management ensures that  systems are deployed in a consistent manner that is known to be  secure. Imaging is a common configuration management technique  that ensures that systems start with a known baseline. Change  management helps reduce unintended outages from unauthorized  changes and can also help prevent changes from weakening security. \nPatch and vulnerability management procedures work together to  keep systems protected against known vulnerabilities. Patch  management keeps systems up-to-date with relevant patches. \nVulnerability management includes vulnerability scans to check for a  wide variety of known vulnerabilities (including unpatched systems). \nStudy Essentials \nKnow the difference between need-to-know and the least  privilege principle. Need-to-know and the least privilege  principle are two standard IT security principles implemented in  secure networks. They limit access to data and systems so that users  and other subjects can access only what they require. This limited  access helps prevent security incidents and helps limit the scope of  incidents when they occur. When these principles are not followed,  security incidents result in far greater damage to an organization. \nUnderstand segregation of duties and job rotation. \nSegregation of duties (SoD) is a basic security principle that ensures  that no single person can control all critical functions or system  elements. With job rotation, employees are rotated into different  jobs, or tasks are assigned to different employees. Collusion is an  agreement among multiple persons to perform some unauthorized or  illegal actions. Implementing these policies helps prevent fraud by  limiting actions individuals can do without colluding with others. \nKnow about monitoring privileged operations. Privileged  entities are trusted, but they can abuse their privileges. Because of  this, it's essential to monitor all assignment of privileges and the use  of privileged operations. The goal is to ensure that trusted employees  do not abuse the special privileges they are granted. Monitoring  these operations can also detect many attacks because attackers  commonly use special privileges during an attack. Advanced  privileged account management practices can limit the time users  have advanced privileges. \nUnderstand service-level agreements. Organizations use  service-level agreements (SLAs) with outside entities such as  vendors. They stipulate performance expectations such as maximum  downtimes and often include penalties if the vendor doesn't meet  expectations. \nDescribe personnel safety and security concerns. Duress  systems allow guards to raise alarms in response to emergencies, and  emergency management plans help the organization respond to  disasters. When employees travel, employees need to be aware of the  risks, especially if they travel to different counties. Safety training  and awareness programs ensure employees know about these risks  and ways to mitigate them. \nUnderstand secure provisioning concepts. Secure  provisioning of resources includes ensuring that resources are  deployed in a secure manner and are maintained in a secure manner  throughout their life cycles. Asset management tracks tangible assets \n(hardware and software) and intangible assets (such as patents,  trademarks, the company's goodwill, and copyrights). \nKnow how to manage and protect media. Media  management techniques track media used to hold sensitive data. \nMedia is protected throughout its lifetime and destroyed when it's no  longer needed. \nKnow the difference between SaaS, PaaS, and IaaS. \nSoftware as a service (SaaS) models provide fully functional  applications typically accessible via a web browser. Platform as a  service (PaaS) models provide consumers with a computing  platform, including hardware, operating systems, and a runtime  environment. Infrastructure as a service (IaaS) models provide basic  computing resources such as servers, storage, and networking  resources. \nKnow about serverless architecture. Serverless architecture  is a cloud computing concept where code is managed by the  customer, and the platform (i.e., supporting hardware and software)  or server is managed by the cloud service provider (CSP). There is  always a physical server running the code, but this execution model  allows the software designer/architect/programmer/developer to  focus on the logic of their code and not have to be concerned about  the parameters or limitations of a specific server. This is also known  as function as a service (FaaS). \nRecognize security issues with managed services in the  cloud. Managed services in the cloud include any resources stored  in or accessed via the cloud. Storing data in the cloud increases the  risk, so additional steps may be necessary to protect the data,  depending on its value. When leasing cloud-based services, you must  understand who is responsible for maintenance and security. The  cloud service provider provides the least amount of maintenance and  security in the IaaS model. \nExplain configuration and change control management. \nMany outages and incidents can be prevented with effective  configuration and change management programs. Configuration  management (CM) ensures that systems are configured similarly and  the configurations of systems are known and documented. \nBaselining ensures that systems are deployed with a common  baseline or starting point, and imaging is a common baselining  method. Change management helps reduce outages or weakened  security from unauthorized changes. A CM process requires changes  to be requested, reviewed, approved, tested, scheduled and  implemented, and documented. Versioning uses a labeling or  numbering system to track changes in updated versions of software. \nUnderstand patch management. Patch management ensures  that systems are kept up-to-date with current patches. You should  know that an effective patch management program will evaluate,  test, approve, and deploy patches. Additionally, be aware that system  audits verify the deployment of approved patches to systems. Patch  management is often intertwined with change and configuration  management to ensure that documentation reflects the changes. \nWhen an organization does not have an effective patch management  program, it will often experience outages and incidents from known  issues that could have been prevented. \nExplain vulnerability management. Vulnerability  management includes routine vulnerability scans and periodic  vulnerability assessments. Vulnerability scanners can detect known  security vulnerabilities and weaknesses such as the absence of  patches or weak passwords. They generate reports that indicate the  technical vulnerabilities of a system and are an effective check for a  patch management program. Vulnerability assessments extend  beyond just technical scans and can include reviews and audits to  detect vulnerabilities. \nWritten Lab \n1. Define the difference between need-to-know and the least  privilege principle. \n2. Describe the purpose of monitoring the assignment and usage of  special privileges. \n3. List the three primary cloud-based service models and identify  the level of maintenance provided by the cloud service provider  in each of the models. \n4. Explain how change management processes help prevent  outages. \nReview Questions \n1. Which security principle involves the knowledge and possession  of sensitive material as an aspect of one's occupation? \nA. Principle of least privilege \nB. Segregation of duties \nC. Need-to-know \nD. As-needed basis \n2. An organization ensures that users are granted access to only the  data they need to perform specific work tasks. What principle  are they following? \nA. Principle of least permission \nB. Segregation of duties (SoD) \nC. Need-to-know \nD. Job rotation \n3. What concept is used to grant users only the rights and  permissions they need to complete their job responsibilities? \nA. Need-to-know \nB. Mandatory vacations \nC. Least privilege principle \nD. Service-level agreement (SLA) \n4. A large organization using a Microsoft domain wants to limit the  amount of time users have elevated privileges. Which of the  following security operation concepts can be used to support  this goal? \nA. Principle of least permission \nB. Segregation of duties \nC. Need-to-know \nD. Privileged account management \n5. An administrator is granting permissions to a database. What is  the default level of access the administrator should grant to new  users in the organization? \nA. Read \nB. Modify \nC. Full access \nD. No access \n6. You want to apply the least privilege principle when creating  new accounts in the software development department. Which  of the following should you do? \nA. Create each account with only the rights and permissions  needed by the employee to perform their job. \nB. Give each account full rights and permissions to the servers  in the software development department. \nC. Create each account with no rights and permissions. \nD. Add the accounts to the local Administrators group on the  new employee's computer. \n7. Your organization has divided a high-level auditing function into  several individual job tasks. These tasks are divided between  three administrators. None of the administrators can perform all  of the tasks. What does this describe? \nA. Job rotation \nB. Mandatory vacation \nC. Segregation of duties \nD. Least privilege \n8. A financial organization commonly has employees switch duty  responsibilities every six months. What security principle are  they employing? \nA. Job rotation \nB. Segregation of duties \nC. Mandatory vacations \nD. Least privilege \n9. Which of the following is one of the primary reasons an  organization enforces a mandatory vacation policy? \nA. To rotate job responsibilities \nB. To detect fraud \nC. To increase employee productivity \nD. To reduce employee stress levels \n10. Your organization has contracted with a third-party provider to  host cloud-based servers. Management wants to ensure there  are monetary penalties if the third party doesn't meet their  contractual responsibilities related to uptimes and downtimes. \nWhich of the following is the best choice to meet this  requirement? \nA. MOU \nB. ISA \nC. SLA \nD. SED \n11. Which one of the following is a cloud-based service model that  gives an organization the most control and requires the  organization to perform all maintenance on operating systems  and applications? \nA. Infrastructure as a service (IaaS) \nB. Platform as a service (PaaS) \nC. Software as a service (SaaS) \nD. Public \n12. Which one of the following is a cloud-based service model that  allows users to access email via a web browser? \nA. Infrastructure as a service (IaaS) \nB. Platform as a service (PaaS) \nC. Software as a service (SaaS) \nD. Public \n13. The IT department routinely uses images when deploying new  systems. Of the following choices, what is a primary benefit of  using images? \nA. Provides baseline for configuration management \nB. Improves patch management response times \nC. Reduces vulnerabilities from unpatched systems \nD. Provides documentation for changes \n14. A server administrator recently modified the configuration for a  server to improve performance. Unfortunately, when an  automated script runs once a week, the modification causes the  server to reboot. It took several hours of troubleshooting to  ultimately determine the problem wasn't with the script but  instead with the modification. What could have prevented this? \nA. Vulnerability management \nB. Patch management \nC. Change management \nD. Blocking all scripts \n15. Which of the following steps would be included in a change  management process? (Choose three.) \nA. Immediately implement the change if it will improve  performance. \nB. Request the change. \nC. Create a rollback plan for the change. \nD. Document the change. \n16. A new CIO learned that an organization doesn't have a change  management program. She insists one be implemented  immediately. Of the following choices, what is a primary goal of  a change management program? \nA. Personnel safety \nB. Allowing rollback of changes \nC. Ensuring that changes do not reduce security \nD. Auditing privilege access \n17. Systems within an organization are configured to receive and  apply patches automatically. After receiving a patch, 55 of the  systems automatically restarted and booted into a stop error. \nWhat could have prevented this problem without sacrificing  security? \nA. Disable the setting to apply the patches automatically. \nB. Implement a patch management program to approve all  patches. \nC. Ensure systems are routinely audited for patches. \nD. Implement a patch management program that tests patches  before deploying them. \n18. A security administrator wants to verify the existing systems are  up-to-date with current patches. Of the following choices, what  is the best method to ensure systems have required patches? \nA. Patch management system \nB. Patch scanner \nC. Penetration tester \nD. Fuzz tester \n19. A recent attack on servers within your organization caused an  excessive outage. You need to check systems for known issues  that attackers may use to exploit other systems in your network. \nWhich of the following is the best choice to meet this need? \nA. Versioning tracker \nB. Vulnerability scanner \nC. Security audit \nD. Security review \n20. Which one of the following processes is most likely to list all  known security risks within a system? \nA. Configuration management \nB. Patch management \nC. Hardware inventory \nD. Vulnerability scan ",
  "elements": {
    "summaries": [
      "Several basic security principles are at the core of security operations in any environment. These include need-to-know, least privilege, segregation of duties (SoD) and responsibilities, job rotation and mandatory vacations, privileged account management, and service- level agreements (SLAs). Combined, these practices help prevent security incidents from occurring and limit the scope of incidents that do occur. When addressing personnel safety and security, safety of personnel should always be a high priority. Duress systems allow guards to raise silent alarms in response to emergencies, and emergency management plans help the organization respond to disasters. Traveling presents unique risks to employees, such as the loss of data, malware installed on unattended systems, and intercepted data when using free Wi-Fi networks. Safety training and awareness programs ensure that personnel know the various risks and ways to mitigate them. Training and awareness programs should also address other critical issues, including the insider threat, social media use, and multifactor authentication fatigue. Asset management extends beyond media to any asset considered valuable to an organization. This includes both tangible and intangible assets. Tangible assets include hardware and software, and organizations commonly inventory these assets to track them. Intangible assets include patents, trademarks, and copyrights, and organizations track these assets as well. With resource protection, media and other assets that contain data are protected throughout their life cycle. Media includes anything that can hold data, such as tapes, internal drives, portable drives, CDs and DVDs, mobile devices, memory cards, and printouts. Media holding sensitive information should be marked, handled, stored, and destroyed using methods that are acceptable within the organization. Managed services in the cloud include any resources stored in or accessed via the cloud. When negotiating with cloud service providers, you must understand who is responsible for maintenance and security. In general, the cloud service provider has the most responsibility with software as a service (SaaS) resources, less responsibility with platform as a service (PaaS) offerings, and the least responsibility with infrastructure as a service (IaaS) offerings. Cloud services commonly provide elasticity, which is the ability of services to dynamically respond to changing workload requirements. Change and configuration management are two additional controls that help reduce outages. Configuration management ensures that systems are deployed in a consistent manner that is known to be secure. Imaging is a common configuration management technique that ensures that systems start with a known baseline. Change management helps reduce unintended outages from unauthorized changes and can also help prevent changes from weakening security. Patch and vulnerability management procedures work together to keep systems protected against known vulnerabilities. Patch management keeps systems up-to-date with relevant patches. Vulnerability management includes vulnerability scans to check for a wide variety of known vulnerabilities (including unpatched systems). Study Essentials Know the difference between need-to-know and the least privilege principle. Need-to-know and the least privilege principle are two standard IT security principles implemented in secure networks. They limit access to data and systems so that users and other subjects can access only what they require. This limited access helps prevent security incidents and helps limit the scope of incidents when they occur. When these principles are not followed, security incidents result in far greater damage to an organization. Understand segregation of duties and job rotation. Segregation of duties (SoD) is a basic security principle that ensures that no single person can control all critical functions or system elements. With job rotation, employees are rotated into different jobs, or tasks are assigned to different employees. Collusion is an agreement among multiple persons to perform some unauthorized or illegal actions. Implementing these policies helps prevent fraud by limiting actions individuals can do without colluding with others. Know about monitoring privileged operations. Privileged entities are trusted, but they can abuse their privileges. Because of this, it's essential to monitor all assignment of privileges and the use of privileged operations. The goal is to ensure that trusted employees do not abuse the special privileges they are granted. Monitoring these operations can also detect many attacks because attackers commonly use special privileges during an attack. Advanced privileged account management practices can limit the time users have advanced privileges. Understand service-level agreements. Organizations use service-level agreements (SLAs) with outside entities such as vendors. They stipulate performance expectations such as maximum downtimes and often include penalties if the vendor doesn't meet expectations. Describe personnel safety and security concerns. Duress systems allow guards to raise alarms in response to emergencies, and emergency management plans help the organization respond to disasters. When employees travel, employees need to be aware of the risks, especially if they travel to different counties. Safety training and awareness programs ensure employees know about these risks and ways to mitigate them. Understand secure provisioning concepts. Secure provisioning of resources includes ensuring that resources are deployed in a secure manner and are maintained in a secure manner throughout their life cycles. Asset management tracks tangible assets (hardware and software) and intangible assets (such as patents, trademarks, the company's goodwill, and copyrights). Know how to manage and protect media. Media management techniques track media used to hold sensitive data. Media is protected throughout its lifetime and destroyed when it's no longer needed. Know the difference between SaaS, PaaS, and IaaS. Software as a service (SaaS) models provide fully functional applications typically accessible via a web browser. Platform as a service (PaaS) models provide consumers with a computing platform, including hardware, operating systems, and a runtime environment. Infrastructure as a service (IaaS) models provide basic computing resources such as servers, storage, and networking resources. Know about serverless architecture. Serverless architecture is a cloud computing concept where code is managed by the customer, and the platform (i.e., supporting hardware and software) or server is managed by the cloud service provider (CSP). There is always a physical server running the code, but this execution model allows the software designer/architect/programmer/developer to focus on the logic of their code and not have to be concerned about the parameters or limitations of a specific server. This is also known as function as a service (FaaS). Recognize security issues with managed services in the cloud. Managed services in the cloud include any resources stored in or accessed via the cloud. Storing data in the cloud increases the risk, so additional steps may be necessary to protect the data, depending on its value. When leasing cloud-based services, you must understand who is responsible for maintenance and security. The cloud service provider provides the least amount of maintenance and security in the IaaS model. Explain configuration and change control management. Many outages and incidents can be prevented with effective configuration and change management programs. Configuration management (CM) ensures that systems are configured similarly and the configurations of systems are known and documented. Baselining ensures that systems are deployed with a common baseline or starting point, and imaging is a common baselining method. Change management helps reduce outages or weakened security from unauthorized changes. A CM process requires changes to be requested, reviewed, approved, tested, scheduled and implemented, and documented. Versioning uses a labeling or numbering system to track changes in updated versions of software. Understand patch management. Patch management ensures that systems are kept up-to-date with current patches. You should know that an effective patch management program will evaluate, test, approve, and deploy patches. Additionally, be aware that system audits verify the deployment of approved patches to systems. Patch management is often intertwined with change and configuration management to ensure that documentation reflects the changes. When an organization does not have an effective patch management program, it will often experience outages and incidents from known issues that could have been prevented. Explain vulnerability management. Vulnerability management includes routine vulnerability scans and periodic vulnerability assessments. Vulnerability scanners can detect known security vulnerabilities and weaknesses such as the absence of patches or weak passwords. They generate reports that indicate the technical vulnerabilities of a system and are an effective check for a patch management program. Vulnerability assessments extend beyond just technical scans and can include reviews and audits to detect vulnerabilities."
    ],
    "reviewQuestions": [],
    "writtenLabs": []
  }
}